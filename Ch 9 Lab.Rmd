---
title: "Ch 9 Lab"
output: html_document
---

Question(4)
```{r}
library(e1071)
set.seed(1)
x=sort(rnorm(100,1,2))
y=rnorm(100)
f <- function(x) 0.5*(x+0.5)*(x-1)*(x-3)

lab=factor(ifelse(y>f(x), "black", "red"))
plot(x,y,col=lab)
lines(x,f(x))
```


```{r}
set.seed(1)
train=sample(100, 50)
dat=data.frame(x,lab)
svmfit=svm(lab~., data=dat[train,], kernel="linear", cost=10)
preds=predict(svmfit, newdata=dat[-train,])
table(preds, dat[-train, "lab"])
```

```{r}
svmfit=svm(lab~., data=dat[train,], kernel="polynomial", cost=10)
preds=predict(svmfit, newdata=dat[-train,])
table(preds, dat[-train, "lab"])
```

```{r}
svmfit=svm(lab~., data=dat[train,], kernel="radial", cost=10)
preds=predict(svmfit, newdata=dat[-train,])
table(preds, dat[-train, "lab"])
```

Linear svm makes most errors, poly only improves by 2 predictions, but radial is the most accurate model that does far better.

Question(5)
```{r}
x1=runif(500)-0.5
x2=runif(500)-0.5
yn=1*(x1^2-x2^2>0)
y=factor(yn)
df = data.frame(x1, x2, y)
```

part(b)
```{r}
plot(x1,x2, col=yn+1)
```

part(c)
```{r}
library(class)
set.seed(1)

glm.fit=glm(y~., data=df, family="binomial")
summary(glm.fit)
```
part(d)
```{r}
preds=predict(glm.fit, type="response")
predscol=ifelse(preds>0.5, "blue", "red")
plot(x1, x2, col=predscol)

```



```{r}
glm.fit=glm(y~poly(x1, 2)+poly(x2, 2), data=df, family="binomial")
summary(glm.fit)

```


```{r}
preds=predict(glm.fit, type="response")
predscol=ifelse(preds>0.5, "blue", "red")
plot(x1, x2, col=predscol)
```

part(g)
```{r}
set.seed(1)
svmfit=svm(y~., data=df[train,], kernel="linear")
summary(svmfit)
```

part(h)
```{r}
preds=predict(svmfit)
predscol=ifelse(preds==1, "blue", "red")
plot(x1, x2, col=predscol)
```

```{r}
set.seed(1)
svm.mod=svm(y~x1+x2, data=dat, kernel="polynomial", d=2, cost=1)
svm.pred=predict(svm.mod)
plot(x1, x2, col=svm.pred)
```

```{r}
set.seed(1)
svm.mod=svm(y~x1+x2, data=dat, kernel="radial", cost=1)
svm.pred=predict(svm.mod)
plot(x1, x2, col=svm.pred)
```

part(i)
Logistic regression with polynomial, svm with radial and polynomial kernel seem to do better.


Question(6)
```{r}
set.seed(1)
x1 = runif(5000) - 0.5
x2 = runif(5000) - 0.5
```



```{r}
yn = 1 * (abs(x1) > abs(x2) + 2*sin(x2))
y = factor(yn)
df=data.frame(y, x1, x2)
plot(x1, x2, col=yn+1)
```

```{r}
set.seed(1)
train=sample(5000, 3500)
tune.out=tune(svm, y~., data=df[train,], kernel="linear", ranges=list(cost=c(0.1, 1, 10, 100)), gamma=c(0.5, 1))

```

```{r}
summary(tune.out)
```


For cost=0.1, 9 errors misclassified (0.02571429*350), similarly for cost=1, 11 and for cost=10, 10

```{r}
tune.test=tune(svm, y~., data=df[-train,], kernel="linear", ranges=list(cost=c(0.1, 1, 10, 100)))

```


```{r}
set.seed(1)
x1=runif(3000, 0, 90)
x2=runif(3000, 0, 90)

y=ifelse(0.3*x1+33>x2, "blue", "red")

epi=which(x1>77)
for (i in epi){
  if ((0.3*x1[i] < x2[i]-25)&&(x2[i]<60) ) {
      y[i]="red"
  }
}
df.train=data.frame(x1,x2,y)
plot(x1,x2,col=y)

```

```{r}
library(e1071)
set.seed(2)
tune.out=tune(svm, y~., data=df.train, ranges=list(cost=c(0.1, 1, 10, 100)), kernel="linear")
summary(tune.out)
```

63 misclassified for cost 0.1
62 misclassified for cost 1.0, 10.0
60 misclassified for cost 100


Test Data Generation
```{r}
set.seed(313233)
x1=runif(3000, 0, 90)
x2=runif(3000, 0, 90)

y=ifelse(0.3*x1+33>x2, "blue", "red")

epi=which(x1>77)
for (i in epi){
  if ((0.3*x1[i] < x2[i]-25)&&(x2[i]<60) ) {
      y[i]="red"
  }
}
df.test=data.frame(x1,x2,y)

```


```{r}
svm0cost=svm(y~., data=df.train, cost=0.1, kernel="linear")
preds0=predict(svm0cost, df.test)
table(preds0, df.test$y)
```
Total misclassified = False Positive + False Negative = 54


```{r}
preds1=predict(tune.out$best.model, df.test)
table(preds1, df.test$y)
```

Total misclassified = False Positive + False Negative = 57

Model that yeild fewest training error yielded most test error.




Question(7)

```{r}
library(ISLR)
attach(Auto)
set.seed(1)
```

```{r}
mpgmed=1*(mpg>median(mpg))
Auto=data.frame(Auto, mpgmed)
```

```{r}
set.seed(4)
tune.out=tune(svm, mpgmed~cylinders+displacement+horsepower+weight+acceleration, data=Auto, kernel="linear", ranges=list(cost=c(0.1, 1, 10, 100)))
summary(tune.out)
```

The error is decresing at increasing values of cost parameter.



```{r}
library(e1071)
set.seed(2)
costs=10^seq(-1, 3)
gammas=10^seq(-2, 3)
degrees=10^seq(-1, 2)
tune.out=tune(svm, mpgmed~cylinders+displacement+horsepower+weight+acceleration, data=Auto, kernel="linear", ranges=list(cost=costs, gamma=gammas, degree=degrees))
summary(tune.out)
```

```{r}
svmrad=svm(mpgmed~cylinders+displacement+horsepower+weight+acceleration, data=Auto,, cost=0.1, kernel="radial")
preds=predict(svmrad, Auto)
preds=1*ifelse(preds<0.5, 0, 1)
table(preds, Auto$mpgmed)
```

Error is 9.948%

```{r}
svmpoly=svm(mpgmed~cylinders+displacement+horsepower+weight+acceleration, data=Auto,, cost=0.1, kernel="polynomial")
preds=predict(svmpoly, Auto)
preds=ifelse(preds<0.5, 0, 1)
table(preds, Auto$mpgmed)
```

Error is 25.25% for polynomial.

Radial kernel performs better.

```{r}
plotpairs = function(fit) {
    for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpgmed", "name"))]) {
        plot(fit, Auto, as.formula(paste("mpg~", name, sep = "")))
    }
}
plotpairs(svmrad)
```

Question(8)
part(a)
```{r}
set.seed(1)
attach(OJ)
sized=dim(OJ)[1]
train=sample(sized, 800)

```

part(b)
```{r}
svmlinear=svm(Purchase~., data=OJ[train,], kernel="linear", cost=0.01)
summary(svmlinear)
```

part(c)
```{r}
predtrain=predict(svmlinear, newdata=OJ[train,])
table(predtrain, data=OJ[train,"Purchase"])
```



```{r}
predtest=predict(svmlinear, newdata=OJ[-train,])
table(predtest, data=OJ[-train,"Purchase"])
```

Training error rate = (75+65) / 800 = 17.5%
Test error rate     = (33+15) / 270 = 17.78%

part(d)
```{r}
costs=10^seq(-2,1)
tune.out=tune(svm, Purchase~., data=OJ[train,], ranges=list(cost=costs), kernel="linear")
summary(tune.out)
```



part(e)
```{r}
preds=predict(tune.out$best.model, OJ[-train,])
table(preds, OJ[-train, "Purchase"])
```

Training Error = 17.35%
Test Error = 15.55%

part(f)
```{r}
tune.rad=tune(svm, Purchase~., data=OJ[train,], ranges=list(cost=costs), kernel="radial")
preds=predict(tune.rad$best.model, OJ[-train,])
summary(tune.out)

```

```{r}
table(preds, OJ[-train, "Purchase"])
```

Training Error: 17.37%
Test Error: 18.51%


part(g)
```{r}
tune.poly=tune(svm, Purchase~., data=OJ[train,], ranges=list(cost=costs), kernel="polynomial", degree=2)
preds=predict(tune.poly$best.model, OJ[-train,])
summary(tune.poly)

```

```{r}
table(preds, OJ[-train, "Purchase"])
```

Training Error: 18.625%
Test Error: 18.75%

Linear Kernel gave best results

